<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>WeTracker</title>
    <description>A Collaborative, Online Music Creation Suite
</description>
    <link>https://wetracker.github.io</link>
    <atom:link href="https://wetracker.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 06 Jan 2017 20:39:29 +0000</pubDate>
    <lastBuildDate>Fri, 06 Jan 2017 20:39:29 +0000</lastBuildDate>
    <generator>Jekyll v3.2.0</generator>
    
      <item>
        <title>Fully Operational Instrument Editor</title>
        <description>&lt;p&gt;After some frantic “holiday hacking”, I’ve just deployed a major update to the
curent status, the introduction of an instrument editor.&lt;/p&gt;

&lt;p&gt;Until this release, the interface has been squarely defined around song playback
and some editing. I realised that in order to make this a fully self sufficient
editing tool, I needed to address the ability to create and edit instruments
next.&lt;/p&gt;

&lt;p&gt;To support this, I’ve introduced a change to the user interface paradigm that
has been on my plans for some time. You may have noticed that the interface is
made up of independent “widgets” each can be placed and sized independently to
define a completely custom layout. This has now been extended to support
multiple views or tabs. Below the, now fixed, transport controls there is now a
tab interface, allowing you to switch between views. Currently there are two
fixed views, song and instrument editing, future updates will allow arbitrary
numbers of customisable views.&lt;/p&gt;

&lt;p&gt;The new widgets to support instrument editing include a volume and panning
envelope editor, a sample mapping interface, and instrument controls. These can
all be seen in operation in the screenshot below, embedded in the new view.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/images/instrumenteditor.png&quot;&gt;
  &lt;img src=&quot;/images/instrumenteditor.png&quot; style=&quot;width: 100%;&quot; /&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The current test release on Heroku includes this new set of features, so please
pop by and kick the tyres. Operation is pretty simple in most cases. Most
widgets can be scrolled and zoomed with the mouse wheel and left/rigth scrolling
where available. The envelopes can be edited by dragging the points, shift click
will insert a new point at the crosshairs, which automatically adjust to show
where on the current curve a new point will be inserted, and alt-click deletes
the point under the mouse. The sample mapper is similar, shift click will split
a segment at the mouse, the single button in the control panel, “Set” will set
the sample number for the selected segment, dragging on the boundary of a
segment will move it. The graph shows the notes from C-0 to B-7 in octaves, the
current sample number in use in a segment is shown in green, any notes played in
that region will trigger that sample number.&lt;/p&gt;

&lt;p&gt;Lots more to do here I’m sure, but basic operation is there, creating
instruments, loading samples (.wav and .mp3 tested), and mapping keys, enough to
get started creating new instruments.&lt;/p&gt;

&lt;p&gt;Onwards and upwards.&lt;/p&gt;
</description>
        <pubDate>Mon, 19 Dec 2016 17:13:00 +0000</pubDate>
        <link>https://wetracker.github.io/tracker,/fasttracker,/webaudio,/instruments/2016/12/19/instrumenteditor.html</link>
        <guid isPermaLink="true">https://wetracker.github.io/tracker,/fasttracker,/webaudio,/instruments/2016/12/19/instrumenteditor.html</guid>
        
        
        <category>tracker,</category>
        
        <category>fasttracker,</category>
        
        <category>webaudio,</category>
        
        <category>instruments</category>
        
      </item>
    
      <item>
        <title>Major Update to the Player Code!</title>
        <description>&lt;p&gt;While the outstanding audio engine from &lt;a href=&quot;https://github.com/a1k0n&quot;&gt;a1k0n&lt;/a&gt; has
proven to be a perfect starting point, and invaluable in getting the project
kickstarted while testing and building the core user interface elements, I
started to realise some limitations. These limitations are not an indication of
anything wrong with the jsxm engine, but rather a misuse of the code, which is
perfectly suited to playback of .xm files, but not perfect for an authoring tool
which needs different capabilities and controls.&lt;/p&gt;

&lt;p&gt;I have been working on a refactoring of that engine, not a complete replacement,
a lot of the effect processing code, logic and inherent Fasttracker 2 format and
playback knowledge is just as valid in the new system. To explain the changes,
and the reasoning, a short, very limited explanation of how jsxm works.&lt;/p&gt;

&lt;p&gt;In a nutshell, jsxm does everything in a single ScriptProcessorNode, processing
effects, combining samples, applying pitch changes, applying volume changes,
extracting visualisation information, etc. This is very much in line with what
you’d expect from a Fasttracker player, where mimicking the capabilities of the
original engine is paramount. This approach gives ultimate control. The cost is
that it’s difficult to control tracks individually, and it’s quite expensive in
terms of processing in Javascript. While I never actually noticed any
performance issue with jsxm, a testament to the quality of the Javascript, the
lack of track level control did become an issue in WeTracker. So, with a1k0n’s
help, I’ve reworked the internals and moved some of the heavy lifting out into
WebAudio nodes.&lt;/p&gt;

&lt;p&gt;The new engine lookes like this…&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/images/node_graphs.png&quot;&gt;
  &lt;img src=&quot;/images/node_graphs.png&quot; style=&quot;width: 100%;&quot; /&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The engine manages an Instrument object for each instrument in the song, which
contains an array of AudioBuffer objects converted from the samples in the song
on loading. A Javascript scheduler creates lightweight PlayerInstrument objects
during playback to represent notes played in the song during scheduling. These
objects hold the AudioBufferSourceNode, GainNode and StereoPannerNode needed to
process the audio and send it further down the graph. These PlayerInstruments
last as long as necessary to complete the note, as soon as they finish, or are
replaced by another instrument on the same track, the are cleaned up. The output
from the PlayerInstrument goes to a GainNode per track, which affords
direct control over the volume and mute of each track. The output of these is
directed into an AnalyserNode per track, which is read by the monitors to
display FFT curves for the notes playing on that track. The scheduling is based
on the excellent artick by Chris Wilson, &lt;a href=&quot;https://www.html5rocks.com/en/tutorials/audio/scheduling/&quot;&gt;A Tale of Two Clocks - Scheduling Web
Audio with
Precision&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Onwards and upwards.&lt;/p&gt;
</description>
        <pubDate>Tue, 13 Dec 2016 17:06:00 +0000</pubDate>
        <link>https://wetracker.github.io/tracker,/fasttracker,/webaudio/2016/12/13/player.html</link>
        <guid isPermaLink="true">https://wetracker.github.io/tracker,/fasttracker,/webaudio/2016/12/13/player.html</guid>
        
        
        <category>tracker,</category>
        
        <category>fasttracker,</category>
        
        <category>webaudio</category>
        
      </item>
    
      <item>
        <title>New Pattern Editor Method</title>
        <description>&lt;p&gt;On the continuing quest to find the most effective way to build this most
critical aspect of the user interface, the pattern editor, I’ve been trying out
something a little different. Having looked closely at the audio engine code
that a1k0n created for his jsxm FasttrackerII player, I turned my attention to
his decision to use canvas for rendering the user interface. This is much more
low level, not relying (or taking advantage of if you like) the DOM that the
browsers use, but instead drawing each element individually. This approach
appeals to me on many levels, not least, that it harks back to my early days,
developing games and softeare on Amiga and DOS machines, where the norm was to
attack the bitmapped screens directly. Ok, perhaps not quite that far back, it
does have a reasonably feature-rich drawin API, but all the same, it’s far more
manual than the equivalent DOM hackery I’d been using up to now.&lt;/p&gt;

&lt;p&gt;As with the DOM based approach, the general scrolling of a pattern during
playback and editing is quite a simple problem to solve. Although with the
canvas I found it a lot easier to solve, implementing the fixed header row and
timeline column in a matter of 30 minutes or so, as opposed to a couple of days
hacking with the DOM. The problem comes when switching patterns. Redrawing an
entire pattern during playback is costly. With the DOM based pattern editor, I
found myself doing all sorts of trick to reduce the amount of events that were
updated during a pattern render, and to cache pointers to the DOM elements that
needed update. Turns out there are similar, although again simpler, approaches
with the canvas. One such, trick, as a large portion of the ‘events’ in a
pattern, espeicially one with many tracks, are empty, drawing the empty note,
instrument, volume, and effects items individually seems silly. Much better to
render an empty event offscreen separately to another canvas, then just draw
that image to the empty event when needed. This trick alone resulted in enough
speed increase to make the pattern transition almost undetectable on my machine
during playback. There are others I’ll be trying over time, but for now, this is
a good sign.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/images/canvas.png&quot;&gt;&lt;img src=&quot;/images/canvas.png&quot; style=&quot;width: 100%;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I’ve also taken, with permission, the basic monitor scopes code from a1k0n’s
jsxm code and integrated them into the codebase as a module. All this is
currently on a separate branch, ‘pattern_change_tests’, as of the writing of
this post, however, it is very likely to be merged into the master very soon as
the way forward for this element of the interface. Anyone wanting to try it out,
checkout that branch, run with &lt;code class=&quot;highlighter-rouge&quot;&gt;npm run start&lt;/code&gt; and view it in your browser
(preferably Chrome), clicking the play button will play a preloaded XM file and
show the moving pattern editor and scopes.&lt;/p&gt;
</description>
        <pubDate>Fri, 02 Dec 2016 16:42:00 +0000</pubDate>
        <link>https://wetracker.github.io/tracker,/pattern,/jquery,/canvas/2016/12/02/canvas.html</link>
        <guid isPermaLink="true">https://wetracker.github.io/tracker,/pattern,/jquery,/canvas/2016/12/02/canvas.html</guid>
        
        
        <category>tracker,</category>
        
        <category>pattern,</category>
        
        <category>jquery,</category>
        
        <category>canvas</category>
        
      </item>
    
      <item>
        <title>Reboot!</title>
        <description>&lt;p&gt;When I found myself spending more and more time researching ways to sidestep
the features of React to eek more performance out of the interface, I started
to ask myself if this was the best approach for my use case. While I have a
great deal of respect for React, and Angular which I used before that, I’m
increasingly convinced that such intrusive frameworks are not the right
approach to this sort of project. The main reason being that the centerpiece of
this application has to be the audio engine. Any CPU cycles lost to a heavy
user interface framework, updating, or even checking if something needs to be
updated, are cycles that can’t be used to push audio, and reduces the chances
of this tool running on a variety of platforms and browsers.&lt;/p&gt;

&lt;p&gt;In me previous post, &lt;a href=&quot;/tracker,/pattern,/react/2016/11/23/patterneditor.html&quot;&gt;Creating a Simple, Fast Pattern Editor in
HTML&lt;/a&gt;, I described how
I was running some experiments into low latency, high performance rendering of
a pattern editor, the primary component of the interface. This experiment took
on a life of it’s own. The more I realised it was orders of magnitude faster,
and more importantly, predictable, I became more and more convinced this was a
better and more appropriate approach to the user interface of WeTracker. To be
clear, by predictable, I mean that everything that happens in the system
happens in a way that I am intimately aware of and in control of. I’ve been
able to achieve more in the few hours of coding in this methodology than I did
in days with React. Not because there’s anything wrong with React, in fact I’m
certain to work with it again, this has proven to be a useful introduction to
the React framework and I’ve enjoyed working with it. The problem I faced was
not knowing the conditions under which things were happening. Normally, this
would not be an issue, I’ve written large Angular based applications where this
feature is indeed a necessity, as keeping track of lots of dependent changes
would have been unworkable. However, for WeTracker, the most important thing is
to know that only what is needed is being updated, and it’s being updated as
fast as feasibly possible.&lt;/p&gt;

&lt;p&gt;I’m not being a complete luddite in creating this new reboot, I’ve included a
very simple and light signals/slots mechanism that I found, and I’m using
ImmutableJS or state management, much like React/Redux, but I’m applying these
design principles in a controlled way, and moreover in a way that ensures I
have completely clear and understandable picture of what is happening and why.&lt;/p&gt;

&lt;p&gt;As of today, I’ve cleared out the old React code (it’s still there in git if
anyone wants to look) and moved the experiment up to root. This will now be the
basis for ongoing work, and the principle applied at all decision points will
be “if I do it this way, am I still going to be in control, and am I still
going to know what is happening and why?”.&lt;/p&gt;

&lt;p&gt;Onwards and upwards.&lt;/p&gt;
</description>
        <pubDate>Mon, 28 Nov 2016 19:13:00 +0000</pubDate>
        <link>https://wetracker.github.io/tracker,/pattern,/jquery/2016/11/28/reboot.html</link>
        <guid isPermaLink="true">https://wetracker.github.io/tracker,/pattern,/jquery/2016/11/28/reboot.html</guid>
        
        
        <category>tracker,</category>
        
        <category>pattern,</category>
        
        <category>jquery</category>
        
      </item>
    
      <item>
        <title>Creating a Simple, Fast Pattern Editor in HTML</title>
        <description>&lt;p&gt;Probably the second most important element of a good &lt;em&gt;Tracker&lt;/em&gt;, second only to the music engine of course, is a streamlined and effective pattern editor. Many people are drawn to the Tracker approach by the interface’s ability to afford rapid and effective data entry and editing. This is made possible in no small part by the simple and effective pattern editor interface. Tuned over many years on very low end hardware (although not necessarily considered low end at the time) such as the Amiga and DOS PC, this interface is an exercise in streamlined and unobtrusive user interaction.&lt;/p&gt;

&lt;p&gt;As such, I decided that it was imperative when building a web based Tracker, that this part of the interface should be equally effective. Not only that, but in the realm of browser based applications, where you have little control over the host platform, it’s important to eek as much out of the system as possible, and when playing complex audio through Javascript, you don’t want the interface being computationally unweildy and taking away precious compute cycles from the audio engine.&lt;/p&gt;

&lt;p&gt;After some initial experiments in pure React, I got very concerned that the performance of the pattern editor was going to be an issue, so I decided to run some separate exploration to see what could be achieved in isolation, to see if React or other UI elements in use at the time were hampering the potential to achive the low latency, low demand interface I needed.&lt;/p&gt;

&lt;p&gt;I created a simple, single page, proof of concept that exercised the basic building blocks:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Frozen&lt;/em&gt; first column and header row.&lt;/li&gt;
  &lt;li&gt;Fast navigation.&lt;/li&gt;
  &lt;li&gt;Row based scrolling, rather than pixel based.&lt;/li&gt;
  &lt;li&gt;Cycled scrolling (when it reaches the bottom, it returns to the top).&lt;/li&gt;
  &lt;li&gt;Edit cursor.&lt;/li&gt;
  &lt;li&gt;Ideally, all in standard HTML with CSS styling.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This had to be able to render as fast as the browser could go in order to support playback at higher rates, such as 200+ beats per minute (bpm) with 16 or even 32 lines per beat (lpb).&lt;/p&gt;

&lt;p&gt;The result was encouraging, click on the screenshot below to load the experiment and test it for yourself. Only basic interaction is included, mouse wheel to scroll, but you can see that even with frantic scrolling, the display keeps up, running smoothly at 60fps even on low end hardware.
&lt;a href=&quot;/experiments/pattern_editor/index.html&quot;&gt;&lt;img src=&quot;/images/patterneditortest.png&quot; style=&quot;width: 100%;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Next up, I need to translate this into the React framework that is being developed. I’m still seeing significant slowdown, even using this same technique, so there’s work to do, but at least the theory is proven.&lt;/p&gt;
</description>
        <pubDate>Wed, 23 Nov 2016 09:18:00 +0000</pubDate>
        <link>https://wetracker.github.io/tracker,/pattern,/react/2016/11/23/patterneditor.html</link>
        <guid isPermaLink="true">https://wetracker.github.io/tracker,/pattern,/react/2016/11/23/patterneditor.html</guid>
        
        
        <category>tracker,</category>
        
        <category>pattern,</category>
        
        <category>react</category>
        
      </item>
    
  </channel>
</rss>
